\documentclass{article}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{dsfont}
\usepackage{amssymb}
\title{TMA4280 computing parallel sums of vectors.}
\author{Authors}

% Useful commands


\begin{document}
\maketitle

In this project we have studied the vector $v \in \mathbb{R}^n$ where the elements are defined as 
\begin{equation}
\label{vdefinition}
	v(i) =\frac{1}{i^2},\quad i = 1,2,...,n.
\end{equation}
Generating the vector $v$ requires $3n$ floating point operations. For each element in the vector we do one multiplication, one division and one assignment.  
The sum $S_n$ of all the vector elements 
\begin{equation}
\label{Sdefinition}
	S_n = \sum_{i=1}^n v(i)
\end{equation}
requires $n+1$ floating point operations, $n$ for summing all the elements and one for assigning the sum.

First, a single processor C program (sum.c) generating $v$, computing $S_n$ in double precision and computing and printing the difference $S - S_n$ for different values of $n$ was written. Next, the program was changed to utilize shared memorize paralellization through OpenMP (sum\_ omp.c). 

Then, a program computing the sum using $P$ processors and a distributed memory model, as described in (REFERANSE), was made (sum\_ mpi.c). In this program we used the convenient MPI calls MPI\_ Scatter and MPI\_ Reduce. The scatter call were chosen instead of the send/receive calls because this operation was well suited for a one-to-all operation. The scatter call were also used instead of the distribute call since the separate threads don't need all the data (saves memory). The reduce call was used instead of MPI\_ Gather since all we wanted was the sum, and the the gather was not needed. Necessary MPI calls were as usual the MPI\_ Init, MPI\_ Comm\_ size, MPI\_ Comm\_ rank and MPI\_ Finalize. Error values of the sum are presented in table \ref{tab:MPIerror}.

Lastly a program using both MPI and OpenMP were tested (sum\_ omp\_ mpi.c) and confirmed working.

\begin{table}
\centering
\caption{Error values for different values of n using the MPI program with two cores.}
\label{tab:MPIerror}
\begin{tabular}{rr}
$n$ & $S-s_n$  \\ 
\hline 
8     & 1.175120e-01 \\ 
16    & 6.058753e-02 \\ 
32    & 3.076680e-02 \\ 
64    & 1.550357e-02 \\ 
128   & 7.782062e-03 \\ 
256   & 3.898631e-03 \\ 
512   & 1.951219e-03 \\ 
1024  & 9.760858e-04 \\ 
2048  & 4.881621e-04 \\ 
4096  & 2.441108e-04 \\ 
8192  & 1.220629e-04 \\ 
16384 & 6.103329e-05
\end{tabular}
\end{table} 
  
  
The difference in the sum was exactly the same as in table \ref{tab:MPIerror} if we ran the serial code, the MPI code with $P =2$ or $P = 8$ threads. Since the MPI code scatters the data over more threads we don't have control over the order which the elements are summed and is thus not guaranteed an equal result. In this case it worked out well.
 
 
\end{document}
